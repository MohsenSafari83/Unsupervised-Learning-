# -*- coding: utf-8 -*-
"""Customer_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mE__I_4FJcpC8GVb6KG3RqdvbG7E5QxY

---

## ðŸ“‚ About the Dataset  

The dataset consists of **2,240 records** and **29 attributes**, which can be categorized into the following groups:  

### ðŸ‘¤ Customer Information  
- `ID`, `Year_Birth`, `Education`, `Marital_Status`, `Income`, `Kidhome`, `Teenhome`, `Dt_Customer`, `Recency`  

### ðŸ›’ Products (Amount spent in the last 2 years)  
- `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, `MntGoldProds`  

### ðŸŽ¯ Promotion  
- `NumDealsPurchases`: Number of purchases made with a discount  
- `AcceptedCmp1`, `AcceptedCmp2`, `AcceptedCmp3`, `AcceptedCmp4`, `AcceptedCmp5`, `Response`  

### ðŸ¬ Place (Purchasing Channels)  
- `NumWebPurchases`, `NumCatalogPurchases`, `NumStorePurchases`, `NumWebVisitsMonth`  

---

#**Loading and exploring the data**

##**importing the libraries**
"""

# 1. to handle the data
import pandas as pd
import numpy as np

# 2. To Viusalize the data
import matplotlib.pyplot as plt
import seaborn as sns

# 3. To preprocess the data
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

"""##**reading and uploding the dataset**

"""

import os
os.makedirs('images', exist_ok=True)
print("images/ folder created or already exists.")
# for saving the results

df=pd.read_csv('marketing_campaign.csv',sep="\t")

"""##**Dataset Information**"""

df.head()

df.shape

df.columns

df.info()

df.describe().T

df.isnull().sum()

df['Income'].fillna(df['Income'].median(), inplace=True)

df.isnull().sum()

"""**Handling Missing Values**\
During EDA, we found **24 missing values** in the `Income` column.  
Since this represents less than 1.1% of the total dataset, two approaches can be considered:  
- Dropping the records with missing values  
- Replacing them with a statistical measure (e.g., median income)  

For this analysis, we choose to **impute with median** to keep the dataset consistent.

**Data Preprocessing Notes**

- **Date Parsing:**  
  The feature `Dt_Customer`, which indicates the date a customer joined the database, is not yet parsed as a `datetime` object.  
  We will convert it to a proper `datetime` format and may extract additional features such as `year`, `month`, or `customer tenure`.  

- **Categorical Features:**  
  Several features are categorical (`dtype: object`), such as `Education` and `Marital_Status`.  
  These need to be encoded into numeric form before applying machine learning algorithms.  

---
"""

df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], dayfirst=True)
print("Oldest customer date:", df['Dt_Customer'].min())
print("Oldest customer date:", df['Dt_Customer'].max())

"""- Now we will be exploring the unique values in the categorical features to get a clear idea of the data."""

print("Total categories in the feature Marital_Status:\n", df["Marital_Status"].value_counts(), "\n")
print("Total categories in the feature Education:\n", df["Education"].value_counts())

#CHECKING NUMBER OF UNIQUE CATEGORIES PRESENT IN THE "Education"
print("Unique categories present in the Education:",df["Education"].value_counts())
print('\n')

#VISUALIZING THE "Education"
df['Education'].value_counts().plot(kind='bar',color='skyblue',edgecolor = "black",linewidth = 3)
plt.title("Frequency Of Each Category in the Education Variable \n",fontsize=24)
plt.figure(figsize=(8,8))

"""- 97.58% of Customers in the dataset are "Post Graduate".
- 2.410% of Customers in the dataset are "Under Graduate".
"""

df['Marital_Status'].unique()

#REPLACING THE CONFLICT VALUES IN Marital_status..
df['Marital_Status'] = df['Marital_Status'].replace(['Married', 'Together'],'Relationship')
df['Marital_Status'] = df['Marital_Status'].replace(['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd'],'Single')

#CHECKING NUMBER OF UNIQUE CATEGORIES PRESENT IN THE "Marital_Status"
print("Unique categories present in the Marital_Status:",df['Marital_Status'].value_counts())
print("\n")


#VISUALIZING THE "Marital_Status"
df['Marital_Status'].value_counts().plot(kind='bar',color='skyblue',edgecolor = "black",linewidth = 3)
plt.title("Frequency Of Each Category in the Marital_Status Variable \n",fontsize=24)
plt.figure(figsize=(8,8))

"""- 64.46% of Customers in the dataset are in "Relationship".
- 35.53% of Customers in the dataset are "Single".
"""

plt.figure(figsize=(6,4))
sns.histplot(df['Income'], bins=30, kde=True)
plt.title("Income Distribution")
plt.show()

df['Age'] = 2025 - df['Year_Birth']
plt.figure(figsize=(6,4))
sns.histplot(df['Age'], bins=30, kde=True)
plt.title("Customer Age Distribution")
plt.show()

#number of days a customer was engaged with company
#changing "Dt_customer" into timestamp format

df['Dt_Customer']=pd.to_datetime(df.Dt_Customer, dayfirst=True)
#adding a column "Age" in dataframe
df['Age'] = (pd.Timestamp('now').year) - (pd.to_datetime(df['Dt_Customer']).dt.year)
print("Unique categories present in the Age:",df['Age'].value_counts())
print("\n")

#visualizing the "Age"
df['Age'].value_counts().plot(kind='bar',color='skyblue',edgecolor = "black",linewidth = 3)
plt.title("Frequency Of Each Category in the Age Variable \n",fontsize=24)
plt.figure(figsize=(8,8))

"""#**Data Preprocessing**"""

#Label encoding the categorical features
#Get list of categorical variables
cat_cols = df.select_dtypes(include=['object']).columns

le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

print("All features are now numerical")

#Creating a copy of data
ds = df.copy()
# creating a subset of dataframe by dropping the features on deals accepted and promotions
cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']
ds = ds.drop(cols_del, axis=1)
#Scaling
scaler = StandardScaler()
scaler.fit(ds)
scaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns )
print("All features are now scaled")

non_numeric = ds.select_dtypes(exclude=[np.number]).columns.tolist()
print("Non-numeric columns before conversion:", non_numeric)

if 'Dt_Customer' in ds.columns:
    ds['Dt_Customer'] = pd.to_datetime(ds['Dt_Customer'], dayfirst=True, errors='coerce')

scaled_ds.head()

"""#**Clustering**

---

## ðŸ”¹ Clustering Approach

In this section, we will apply clustering techniques to segment customers based on their features.  
A detailed theoretical explanation of **clustering methods (such as KMeans and others)** has already been provided in the accompanying documentation.

**Why DBSCAN Was Not Used**

Although DBSCAN is a popular density-based clustering algorithm, it was not suitable for this dataset due to the following reasons:

1. **Irregular and complex cluster shapes** â€“ While DBSCAN handles arbitrary shapes in some cases, our clusters vary widely in density and shape, which can lead to unstable results.  
2. **High noise and outliers** â€“ The dataset contains many outliers, which DBSCAN may classify as noise, reducing cluster interpretability.  
3. **Unknown number of clusters** â€“ DBSCAN requires careful tuning of `eps` and `min_samples`; with unknown cluster count, this becomes challenging.

> Considering these characteristics, KMeans was chosen as it provides more stable and interpretable clusters for this analysis.

**Choosing the Number of Clusters (Elbow Method)**
   - We use the **Elbow Method** by plotting Within-Cluster-Sum-of-Squares (WCSS) for different k values (1â€“17).
   - The "elbow point" suggests the optimal number of clusters.
   - In our case, the elbow suggests **k = 4**.
"""

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 18):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=50)
    kmeans.fit(scaled_ds)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(range(1, 18), wcss, color='skyblue', marker='*')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.tight_layout()

# Save the plot as an image
plt.savefig('images/elbow.png', dpi=150, bbox_inches='tight')

plt.show()

"""**KMeans Clustering**
   - Apply **KMeans** with the selected k (4).
   - Each customer is assigned to a cluster (`y_kmeans`).
   - The cluster labels are added back to the dataset for further profiling.
"""

# Fitting K-Means to the dataset
kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 50)
y_kmeans = kmeans.fit_predict(scaled_ds)

scaled_ds['Cluster'] = y_kmeans

""" **Dimensionality Reduction with PCA**
   - Our dataset has ~30 features, which makes visualization difficult.
   - We use **PCA (Principal Component Analysis)** to reduce dimensions to **2 principal components** for visualization.
   - Note: PCA is **not used for clustering itself** here (clustering is on full data), only for visualization.
   - Explained variance ratio shows how much information PC1 and PC2 preserve.
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce features to 2D using PCA
pca = PCA(n_components=2)
reduced = pca.fit_transform(scaled_ds)

plt.figure(figsize=(8,6))
scatter = plt.scatter(reduced[:,0], reduced[:,1], c=y_kmeans, cmap="Set1", alpha=0.6)
plt.title("Clusters visualized with PCA")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(scatter, label='Cluster')
plt.tight_layout()
plt.savefig('images/pca_clusters.png', dpi=150, bbox_inches='tight')  # save
plt.show()

# 1. Cumulative Variance Plot
pca_full = PCA().fit(scaled_ds.drop('Cluster', axis=1))

plt.figure(figsize=(8,5))
plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o', color='skyblue')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance vs Number of PCA Components")
plt.axhline(y=0.9, color='r', linestyle='--', label="90% Threshold")
plt.legend()
plt.grid()
plt.show()

# 2. Cluster Profiling (mean of features per cluster)
cluster_profiles = scaled_ds.groupby("Cluster").mean()
print("\nCluster Profiles (mean of features per cluster):\n")
print(cluster_profiles)

# 3. Feature Loadings (feature importance of pc1 and pc2)
pca_2d = PCA(n_components=2).fit(scaled_ds.drop('Cluster', axis=1))
loadings = pd.DataFrame(
    pca_2d.components_.T,
    columns=['PC1', 'PC2'],
    index=scaled_ds.drop('Cluster', axis=1).columns
)

print("\nTop 10 features contributing to PC1:\n", loadings['PC1'].abs().sort_values(ascending=False).head(10))
print("\nTop 10 features contributing to PC2:\n", loadings['PC2'].abs().sort_values(ascending=False).head(10))

import os, re

features_to_plot = [
    "Income",
    "MntWines",
    "MntMeatProducts",
    "NumCatalogPurchases",
    "NumStorePurchases",
    "NumWebPurchases",
    "NumDealsPurchases",
    "Teenhome"
]
for feature in features_to_plot:
    plt.figure(figsize=(6,4))
    sns.barplot(x=cluster_profiles.index, y=cluster_profiles[feature])
    plt.title(f"Average {feature} per Cluster")
    plt.xlabel("Cluster")
    plt.ylabel(f"Mean {feature} (standardized)")
    plt.tight_layout()

    # make a safe filename for saving
    safe = re.sub(r'[^A-Za-z0-9_]+', '_', feature).lower()
    fname = f"images/bar_{safe}.png"
    plt.savefig(fname, dpi=150, bbox_inches='tight')
    plt.show()

plt.figure(figsize=(12,6))
sns.heatmap(cluster_profiles, cmap="viridis", annot=True, fmt=".2f")
plt.title("Cluster Profiles Heatmap")
plt.xlabel("Features")
plt.ylabel("Cluster")
plt.tight_layout()
plt.savefig('images/cluster_profiles_heatmap.png', dpi=150, bbox_inches='tight')
plt.show()

"""# Cluster Profiling â€“ Conclusions

After analyzing the mean values of features for each cluster, we can summarize the customer groups as follows:

| Cluster | Profile Summary |
|---------|-----------------|
| **0** | **Low-income families with children** â†’ High `Kidhome`/`Teenhome`, low income, low spending on wines & meat products. Likely budget-conscious households. |
| **1** | **Medium-low income families with teens** â†’ Slightly below-average income, higher `Teenhome`, low product spending overall. Price-sensitive segment. |
| **2** | **High-income luxury buyers** â†’ Very high income, heavy spenders on wines, gold, and catalog purchases. Few children at home. Prime target for premium campaigns. |
| **3** | **Medium-high income, multi-channel shoppers** â†’ Active in web and store purchases, moderate-to-high income, reasonable spending on wines and gold. Represent a digitally engaged customer base. |

---

##  Key Insights
- **Cluster 2** represents the most valuable customers with high purchasing power â†’ best suited for **premium product marketing**.  
- **Clusters 0 and 1** are more **price-sensitive** and family-oriented â†’ promotions and discounts may work better.  
- **Cluster 3** shows a good balance of spending and online/offline engagement â†’ potential for **cross-channel marketing strategies**.

**Note:** The features shown in the plots and used for cluster interpretation were selected **only for explaining and visualizing cluster differences**. The clustering algorithm (KMeans) was run on **all features**. This feature selection is **not part of the modeling process** and is purely for **cluster profiling**.
"""